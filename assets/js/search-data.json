{
  
    
        "post0": {
            "title": "matplotlib 绘制数据动画",
            "content": "#collapse-hide import numpy as np import pandas as pd import matplotlib.pyplot as plt import matplotlib.animation as animation import matplotlib.ticker as ticker from IPython.display import HTML %matplotlib inline . . &#25955;&#28857;&#22270; . &#25913;&#21464;&#39068;&#33394; . 定义数据： . numPoints = 10 numFrames = 20 colorData = np.random.randint(0,255,size=(numFrames, numPoints)) points = np.random.uniform(0,1,size=(numPoints, 2)) . 更新绘制数据： . def update(i, scat): scat.set_array(colorData[i]) return scat, . 画图: . fig, ax = plt.subplots() scat = ax.scatter(points[:,0], points[:,1], s=100, c=colorData[0]) ani = animation.FuncAnimation(fig, update, frames=numFrames, fargs=(scat,)) . 在jupyter notebook交互式显示动画 . HTML(ani.to_jshtml()) . &lt;/input&gt; Once Loop Reflect &#25913;&#21464;&#22352;&#26631; . numPoints = 10 numFrames = 20 pos = np.random.uniform(low=-5,high=5,size=[1,numPoints,2]) pos = np.repeat(pos, repeats=numFrames,axis=0) delta_pos = np.c_[np.linspace(0,5,numFrames),-np.linspace(0,5,numFrames)][:,None,:] delta_pos = np.repeat(delta_pos,repeats=numPoints, axis=1) pos = pos+delta_pos . def update(i, scat): scat.set_offsets(pos[i]) return scat, . fig, ax = plt.subplots() ax.set_xlim(-5,10) ax.set_ylim(-10,5) scat = ax.scatter(points[:,0], points[:,1], s=100) ani = animation.FuncAnimation(fig, update, frames=numFrames, fargs=(scat,)) . HTML(ani.to_jshtml()) . &lt;/input&gt; Once Loop Reflect HTML(ani.to_html5_video()) . Your browser does not support the video tag. ani.save(&#39;test.gif&#39;, writer=&#39;imagemagick&#39;) . ani.save(&#39;test.mp4&#39;) . &#27668;&#29699; . # Define properties of the &quot;bouncing balls&quot; n = 10 pos = (20 * np.random.sample(n*2) - 10).reshape(n, 2) vel = (0.3 * np.random.normal(size=n*2)).reshape(n, 2) # vel = np.c_[np.ones((n,1)),-np.ones((n,1))] sizes = 100 * np.random.sample(n) + 100 # Colors where each row is (Red, Green, Blue, Alpha). Each can go # from 0 to 1. Alpha is the transparency. colors = np.random.sample([n, 4]) fig, ax = plt.subplots() ax.set_xlim(-10,10) ax.set_ylim(-10,10) def update(i,circles): global pos, vel pos = pos+vel bounce = abs(pos) &gt; 10 # Find balls that are outside walls vel[bounce] = -vel[bounce] # Bounce if outside the walls circles.set_offsets(pos) # Change the positions return circles, # Draw all the circles and return an object ``circles`` that allows # manipulation of the plotted circles. circles = ax.scatter(pos[:,0], pos[:,1], marker=&#39;o&#39;, s=sizes, c=colors) ani = animation.FuncAnimation(fig, update, frames=20, fargs=(circles,)) . HTML(ani.to_jshtml()) . &lt;/input&gt; Once Loop Reflect &#20854;&#20182; . &#26059;&#36716;&#31661;&#22836; . x = np.linspace(-3, 3, 91) t = np.linspace(0, 25, 30) y = np.linspace(-3, 3, 91) X3, Y3, T3 = np.meshgrid(x, y, t) sinT3 = np.sin(2*np.pi*T3 / T3.max(axis=2)[..., np.newaxis]) G = (X3**2 + Y3**2)*sinT3 . fig, ax = plt.subplots() ax.set(xlim=(-4, 4), ylim=(-4, 4)) # Plot every 20th arrow step = 20 x_q, y_q = x[::step], y[::step] # Create U and V vectors to plot U = G[::step, ::step, :] V = np.roll(U, shift=3, axis=2) qax = ax.quiver(x_q, y_q, U[..., 0], V[..., 0], scale=100) def animate(i, qax): qax.set_UVC(U[..., i], V[..., i]) . ani = animation.FuncAnimation(fig, animate, frames=20, fargs=(qax,)) . HTML(ani.to_jshtml()) . &lt;/input&gt; Once Loop Reflect &#26354;&#32447;&#22270;&#36208;&#21183;&#22270; . df = pd.read_csv(&#39;data.csv&#39;, usecols=[&#39;name&#39;, &#39;group&#39;, &#39;year&#39;, &#39;value&#39;]) df.head(3) . name group year value . 0 Agra | India | 1575 | 200.0 | . 1 Agra | India | 1576 | 212.0 | . 2 Agra | India | 1577 | 224.0 | . cities = [&#39;Shanghai&#39;, &#39;New York&#39;] df = df[df.name.isin(cities) &amp; (df.year&lt;2019) &amp; (df.year&gt;1929)][[&#39;name&#39;, &#39;year&#39;, &#39;value&#39;]] . population = [df[df.name==c].sort_values(by=&#39;year&#39;).value.values for c in cities] year = np.linspace(1930, 2018, 89) . fig, ax = plt.subplots() . def update(y, year, population, ax): ax.clear() line1, = ax.plot(year[:y], population[0][:y], color=&#39;b&#39;) line2, = ax.plot(year[:y], population[1][:y], color=&#39;r&#39;) line1.set_marker(&#39;o&#39;) line2.set_marker(&#39;o&#39;) line1.set_markevery([y-1]) line2.set_markevery([y-1]) ax.text(year[y-1]-1, population[0][y-1]+1000, &#39;Shanghai&#39;, size=10, weight=600) ax.text(year[y-1]+2, population[0][y-1]-500, f&#39;{int(population[0][y-1])}&#39;) ax.text(year[y-1]-1, population[1][y-1]+1000, &#39;New York&#39;, size=10, weight=600) ax.text(year[y-1]+2, population[1][y-1]-500, f&#39;{int(population[1][y-1])}&#39;) ax.set_ylim(np.min(population[0]),np.max(population[0])) ax.set_xlim(1929,2019) ax.text(0, 1.02, &#39;Shanghai -- New York&#39;, transform=ax.transAxes, size=18, weight=600, ha=&#39;left&#39;) return ax ani = animation.FuncAnimation(fig, update, 89, fargs=(year, population, ax), interval=100) . HTML(ani.to_html5_video()) . Your browser does not support the video tag. bar chart race . ref:https://towardsdatascience.com/bar-chart-race-in-python-with-matplotlib-8e687a5c8a41 | . 以年为单位绘制1968年到2018年人口数目前十的世界城市变化图。 . df = pd.read_csv(&#39;data.csv&#39;, usecols=[&#39;name&#39;, &#39;group&#39;, &#39;year&#39;, &#39;value&#39;]) df.head(3) . name group year value . 0 Agra | India | 1575 | 200.0 | . 1 Agra | India | 1576 | 212.0 | . 2 Agra | India | 1577 | 224.0 | . colors = dict(zip( [&#39;India&#39;,&#39;Europe&#39;,&#39;Asia&#39;,&#39;Latin America&#39;,&#39;Middle East&#39;,&#39;North America&#39;,&#39;Africa&#39;], [&#39;#adb0ff&#39;, &#39;#ffb3ff&#39;, &#39;#90d595&#39;, &#39;#e48381&#39;, &#39;#aafbff&#39;, &#39;#f7bb5f&#39;, &#39;#eafb50&#39;] )) group_lk = df.set_index(&#39;name&#39;)[&#39;group&#39;].to_dict() . def draw_barchart(year, ax): dff = df[df[&#39;year&#39;].eq(year)].sort_values(by=&#39;value&#39;, ascending=True).tail(10) ax.clear() ax.barh(dff[&#39;name&#39;], dff[&#39;value&#39;], color=[colors[group_lk[x]] for x in dff[&#39;name&#39;]]) dx = dff[&#39;value&#39;].max() / 200 for i, (value, name) in enumerate(zip(dff[&#39;value&#39;], dff[&#39;name&#39;])): ax.text(value-dx, i, name, size=14, weight=600, ha=&#39;right&#39;, va=&#39;bottom&#39;) ax.text(value-dx, i-.25, group_lk[name], size=10, color=&#39;#444444&#39;, ha=&#39;right&#39;, va=&#39;baseline&#39;) ax.text(value+dx, i, f&#39;{value:,.0f}&#39;, size=14, ha=&#39;left&#39;, va=&#39;center&#39;) # ... polished styles ax.text(1, 0.4, year, transform=ax.transAxes, color=&#39;#777777&#39;, size=46, ha=&#39;right&#39;, weight=800) ax.text(0, 1.06, &#39;Population (thousands)&#39;, transform=ax.transAxes, size=12, color=&#39;#777777&#39;) ax.xaxis.set_major_formatter(ticker.StrMethodFormatter(&#39;{x:,.0f}&#39;)) ax.xaxis.set_ticks_position(&#39;top&#39;) ax.tick_params(axis=&#39;x&#39;, colors=&#39;#777777&#39;, labelsize=12) ax.set_yticks([]) ax.margins(0, 0.01) ax.grid(which=&#39;major&#39;, axis=&#39;x&#39;, linestyle=&#39;-&#39;) ax.set_axisbelow(True) ax.text(0, 1.12, &#39;The most populous cities in the world from 1500 to 2018&#39;, transform=ax.transAxes, size=24, weight=600, ha=&#39;left&#39;) ax.text(1, 0, &#39;by @matplotlib&#39;, transform=ax.transAxes, ha=&#39;right&#39;, color=&#39;#777777&#39;, bbox=dict(facecolor=&#39;cyan&#39;, alpha=0.8, edgecolor=&#39;white&#39;)) plt.box(False) return ax, . fig, ax = plt.subplots(figsize=(15, 8)) animator = animation.FuncAnimation(fig, draw_barchart, fargs=(ax,), frames=range(1968, 2019)) . HTML(animator.to_html5_video()) . Your browser does not support the video tag.",
            "url": "https://leiyu-thunder.github.io/myblog/matplotlib/2020/06/11/matplotlib-animation.html",
            "relUrl": "/matplotlib/2020/06/11/matplotlib-animation.html",
            "date": " • Jun 11, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Python常用模块 Itertools",
            "content": "python常用模块笔记：itertools . itertools模块提供了一系列被称作”iterator algebra”的迭代方法，能够简洁、高效的实现一些非常有用的数据迭代操作。简单说，使用”iterator algebra”的原因就是：高效、快速。 . iter应用举例：从一个列表中依次取出固定数目的元素 . def better_grouper(inputs, n): iters = [iter(inputs)]*n return zip(*iters) . 原理： . 列表复制的方法首先让我们获得了inputs的n份引用拷贝对象。然后在zip函数中，当从iters中的第一个iter对象中获取了第一个元素后，第二个iter的第一个元素实际上变成了第二个元素，依次类推，从而实现了连续取出固定数目的元素。 . 上面的better_grouper有个问题，如果inputs的长度不是n的整数倍，最后剩余的少于n的元素不能获得，因为zip是以最短的iter对象为准，当zip中的一个iter对象没有元素时，zip自动停止。如果想获得全部元素，可以使用zip_longest()函数。zip_longest函数有个fillvalue参数，可以指定对于缺失值自动填充特定元素。 . itertools 常用函数 . 获取元素组合 . combinations | . 生成可迭代对象元素的不重复组合。 . &gt;&gt;&gt; combinations([1,2,3], 2) (1,2), (1,3), (2,3) . combinations_with_replacement | . 生成可迭代对象的元素组合（元素可重复）。 . &gt;&gt;&gt; combinations_with_replacement([1, 2], 2) (1, 1), (1, 2), (2, 2) . permutations | . 生成可迭代对象的有序组合。 . &gt;&gt;&gt; permutations(&#39;abc&#39;) (&#39;a&#39;, &#39;b&#39;, &#39;c&#39;), (&#39;a&#39;, &#39;c&#39;, &#39;b&#39;), (&#39;b&#39;, &#39;a&#39;, &#39;c&#39;), (&#39;b&#39;, &#39;c&#39;, &#39;a&#39;), (&#39;c&#39;, &#39;a&#39;, &#39;b&#39;), (&#39;c&#39;, &#39;b&#39;, &#39;a&#39;) . product | . 生成可迭代对象的笛卡尔积。 . &gt;&gt;&gt; product([1, 2], [&#39;a&#39;, &#39;b&#39;]) (1,&#39;a&#39;), (1, &#39;b&#39;), (2, &#39;a&#39;), (2, &#39;b&#39;) . 无限循环生成元素 . count | . count(start=0, step=1)函数接收两个参数，能够从start开始，按照步长step不断产生新的元素。 . &gt;&gt;&gt; count() 0, 1, 2, 3, 4, ... &gt;&gt;&gt; count(start=1, step=2) 1, 3, 5, 7, 9, ... . cycle | . cycle(iterable)可以把iterable中的元素循环重复下去。 . &gt;&gt;&gt; cycle([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]) a, b, c, a, b, c, a, ... . repeat | . repeat(object, times=1)将不断重复产生object，重复次数取决于times参数。 . &gt;&gt;&gt; repeat(2, 5) 2, 2, 2, 2, 2 . 串联多个可迭代对象 . chain | . chain(*iterables) 该函数创建一个新的迭代器，会将参数中的所有迭代器全包含进去。 . &gt;&gt;&gt; list(it.chain(&#39;ABC&#39;, &#39;DEF&#39;)) [&#39;A&#39; &#39;B&#39; &#39;C&#39; &#39;D&#39; &#39;E&#39; &#39;F&#39;] &gt;&gt;&gt; list(it.chain([1, 2], [3, 4, 5, 6], [7, 8, 9])) [1, 2, 3, 4, 5, 6, 7, 8, 9] . 元素分组 . groupby | . groupby(iterable, key=None) 分组函数，将 key 函数作用于序列的各个元素。根据 key 函数的返回值将拥有相同返回值的元素分到一个新的迭代器。类似于 SQL 中的 GROUP BY 操作，唯一不同的是该函数对序列的顺序有要求，因为当 key 函数的返回值改变时，迭代器就会生成一个新的分组。因此在使用该函数之前需要先使用同一个排序函数对该序列进行排序操作。 . &gt;&gt;&gt; data = [{&#39;name&#39;: &#39;Alan&#39;, &#39;age&#39;: 34}, ... {&#39;name&#39;: &#39;Catherine&#39;, &#39;age&#39;: 34}, ... {&#39;name&#39;: &#39;Betsy&#39;, &#39;age&#39;: 29}, ... {&#39;name&#39;: &#39;David&#39;, &#39;age&#39;: 33}] ... &gt;&gt;&gt; grouped_data = it.groupby(data, key=lambda x: x[&#39;age&#39;]) &gt;&gt;&gt; for key, grp in grouped_data: ... print(&#39;{}: {}&#39;.format(key, list(grp))) ... 34: [{&#39;name&#39;: &#39;Alan&#39;, &#39;age&#39;: 34}, {&#39;name&#39;: &#39;Betsy&#39;, &#39;age&#39;: 34}] 29: [{&#39;name&#39;: &#39;Catherine&#39;, &#39;age&#39;: 29}] 33: [{&#39;name&#39;: &#39;David&#39;, &#39;age&#39;: 33}] . 实际使用中记得先排序后分组 . def sort_and_group(iterable, key=None): return it.groupby(sorted(iterable, key=key), key=key) . 元素选择 . takewile | . takewhile(predicate, iterable)创建一个迭代器，当遇到predicate为false的时候停止迭代元素。 . it.takewhile(lambda x: x &lt; 3, [0, 1, 2, 3, 4]) # 0, 1, 2 . dropwhile | . dropwhile(predicate, iterable)创建一个迭代器，当predicate首次为false的时候开始迭代元素。 . it.dropwhile(lambda x: x &lt; 3, [0, 1, 2, 3, 4]) # 3, 4 . filterfalse | . filterfalse(predicate, iterable) 创建一个迭代器，返回 iterable 中 predicate 为 false 的元素。 . &gt;&gt;&gt; only_positives = it.filterfalse(lambda x: x &lt;= 0, [0, 1, -1, 2, -2]) &gt;&gt;&gt; list(only_positives) [1, 2] . islice | . islice(iterable, start, stop[, step]) 对 iterable 进行切片操作。从 start 开始到 stop 截止，同时支持以步长为 step 的跳跃。 . &gt;&gt;&gt; # Slice from index 2 to 4 &gt;&gt;&gt; list(it.islice(&#39;ABCDEFG&#39;, 2, 5))) [&#39;C&#39; &#39;D&#39; &#39;E&#39;] &gt;&gt;&gt; # Slice from beginning to index 4, in steps of 2 &gt;&gt;&gt; list(it.islice([1, 2, 3, 4, 5], 0, 5, 2)) [1, 3, 5] &gt;&gt;&gt; # Slice from index 3 to the end &gt;&gt;&gt; list(it.islice(range(10), 3, None)) [3, 4, 5, 6, 7, 8, 9] &gt;&gt;&gt; # Slice from beginning to index 3 &gt;&gt;&gt; list(it.islice(&#39;ABCDE&#39;, 4)) [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;] . 主要参考资料： . itertools in python 3, by example |",
            "url": "https://leiyu-thunder.github.io/myblog/python/2020/05/21/python%E5%B8%B8%E7%94%A8%E6%A8%A1%E5%9D%97-itertools.html",
            "relUrl": "/python/2020/05/21/python%E5%B8%B8%E7%94%A8%E6%A8%A1%E5%9D%97-itertools.html",
            "date": " • May 21, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Vim使用笔记",
            "content": "vim使用笔记 . 常规文本操作 . 删除、撤销、复制、粘贴等 . u撤销操作 | Ctrl-r重做操作 | :↑在切换上一个命令 | yy复制 | p粘贴（p/P都可以，p是表⽰在当前位置之后，P表⽰在当前位置之前）） | dd删除当前行 | . 各种插入模式 . a → 在光标后插⼊ | o → 在当前⾏后插⼊⼀个新⾏ | O → 在当前⾏前插⼊⼀个新⾏ | . 光标移动 . hjkl(←↓↑→）的移动 . | 0 → 数字零，到⾏头 . | ^ → 到本⾏第⼀个不是blank字符的位置（所谓blank字符就是空格， tab，换⾏，回⻋等） . | $ → 到本⾏⾏尾 . | g_ → 到本⾏最后⼀个不是blank字符的位置。 . | NG → 到第 N ⾏ . | gg → 到第⼀⾏ . | G → 到最后⼀⾏ . | w → 到下⼀个单词的开头 . | e → 到下⼀个单词的结尾 . 如果你认为单词是由默认⽅式，那么就⽤⼩写的e和w。默认上来说，⼀个 单词由字⺟，数字和下划线组成 . 如果你认为单词是由blank字符分隔符，那么你需要使⽤⼤写的E和W。 . | b → 倒退到上一个单词开头 . | % →匹配括号移动 . | *和 #：匹配光标当前所在的单词，移动光标到下⼀个（或上⼀个）匹配单词（*是下⼀个，#是上⼀个）。 . | H → 移动到当前屏幕第一行 . | M → 移动到当前屏幕中间行 . | L → 移动到当前屏幕最后一行 . | . 屏幕滚动 . Ctrl-F →屏幕向下滚动一屏 | Ctrl-B→屏幕向上滚动一屏 | Ctrl-E→屏幕向下滚动一行 | Ctrl-Y→屏幕向上滚动一行 | Ctrl-D→屏幕向下滚动半屏 | Ctrl-U→屏幕向上滚动半屏 | . 文件操作 . :e &lt;path/to/file&gt; → 打开⼀个⽂件 | :w → 存盘 | :saveas &lt;path/to/file&gt; → 另存为 &lt;path/to/file&gt; | :x， ZZ 或 :wq → 保存并退出 (:x 表⽰仅在需要时保存，ZZ不需要输 ⼊冒号并回⻋) :q! → 退出不保存 :qa! 强⾏退出所有的正在编辑的⽂件，就算别的⽂件有更改。 | . 重复命令 . . → (⼩数点) 可以重复上⼀次的命令 | :N&lt;command&gt; → 重复某个命令N次 | . 不同buffer之间跳转 . :bn切换到下一个文件。 | :bp切换到上一个文件。 | :ls列出buffer的列表，带编号。 | :b [N]切换到第N个文件，例如：:b 3切换到第三个文件。 | :b {filename}通过文件名切换到对应的buffer，tab可以补全。 | . 多窗口与窗口跳转 . :sp打开一个新的水平切分窗口。 | :vsplit打开一个新的垂直切分窗口。 | . 打开多个窗口后，以下命令在多窗口之间跳转（类似鼠标上下左右移动）： . Ctrl-W j切换到下一个窗口 | Ctrl-W k切换到上一个窗口 | Ctrl-W h切换到左边窗口 | Ctrl-W l切换到右边窗口 | Ctrl-W w在各个窗口之间轮流切换 | . 代码跳转 . 在目标文件夹下生成代码标签：ctags -R | 通过vim打开某一代码文件，把光标移到要跳转的标识符上，按Ctrl + ]跳转到对应位置，按Ctrl + o退回到跳转前的位置 | 开启代码折叠：:set foldmethod=indent zM 全部折叠 | zR 全部展开 | zc 在当前位置折叠 | zo | za 在当前位置展开 | zC 对当前区域做整体折叠；当前区域可以理解为整个函数 | zO | zA 对当前区域做整体展开 | zj 移动下一个折叠点 | zk 移动到上一个折叠垫 | . | . 文本搜索与替换 . /pattern在文本中按照pattern模式进行搜索，n查找下一个，N查找上一个 | *可以查找光标所在的单词，g*即可查找光标所在单词的字符序列，每次出现前后字符无要求。 | s命令用来查找和替换字符串，语法：:{作用范围}s/{目标}/{替换}/{替换标志} | . ​ 例如:%s/foo/bar/g会在全局范围(%)查找foo并替换为bar，每一行中所有出现都会被替换（g）。 . ​ 不同的作用范围标志： . 命令 作用范围 . :s/foo/bar/g | 当前行 | . :%s/foo/bar/g | 全文 | . :’&lt;,’&gt;s/foo/bar/g | visual模式下的选择区域，在visual模式下选择区域后输入:即可自动补全 | . :.,+2s/foo/bar/g | 当前行与接下来的两行 | . 不同的替换标志：g全局替换 . | 全局命令:g，语法：:[range]global[!]/{pattern}/{command}，简写成:[range]g/pattern/command。 . [range] 指定文本范围，默认为整个文档 . | pattern 在范围 range 内的行如果匹配 pattern，则执行 command | ! 表示取反，也就是不匹配的行，也可以使用 vglobal | command 默认是打印文本 | 整个命令可以理解成，在 range 范围内匹配 patter 的行执行 Ex command。 | 常用的Ex command：d（删除）、m（移动）、t（拷贝）、s（替换） | 删除匹配的行：:g/pattern/d | 删除不匹配的行：:g!/pattern/d | 删除空白行：:g/^$/d | 将匹配的行移到文件末尾：g/pattern/m$ | 复制匹配的行到文件末尾：g/pattern/t$ | . | . 正则表达式 . 正则表达式的元字符必须用反斜杠进行转义才行 | 非贪婪匹配：. {-}，贪婪匹配：.* | . 使用终端 . !{cmd}是vim命令，可以用来执行一条shell命令，命令完成后按任意键返回vim。 | Ctrl+z可以立即挂起当前进程，比如vim，进入shell， 完成一系列命令后，使用fg来返回vim。 | :sh[ell]是vim命令，可以从vim中运行一个shell，完成一系列命令后，通过Ctrl+d结束当前shell并返回vim。 | . vim 插件管理 . 常用的vim插件管理方法是vundle。 | . 其他命令 . Ctrl -缩小文本字号 | Ctrl +放大文本字号 | | .",
            "url": "https://leiyu-thunder.github.io/myblog/vim/2020/02/24/vim%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0.html",
            "relUrl": "/vim/2020/02/24/vim%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0.html",
            "date": " • Feb 24, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "深度学习优化器（pytorch）",
            "content": ". 常用深度学习优化器：理论与pytorch代码 . SGD(Stochastic Gradient Descent) . 随机梯度下降中的”stochastic”表示用小批量数据代表模型梯度更新方向。 . 公式： . θ=θ−η∗∇θJ(θ;xi;yi) theta = theta - eta * nabla_ theta J( theta;x^i;y^i)θ=θ−η∗∇θ​J(θ;xi;yi) . pytorch 代码： . for p in group[&#39;params&#39;]: d_p = p.grad.data p.data.add_(-group[&#39;lr&#39;], d_p) . SGD with momentum . SGD的缺点之一是容易在局部极值点反复震荡，解决方法之一在更新过程中加入历史更新信息。类似于从山上滚下的石头，没有阻力的话动量越来越大，遇到阻力动量变小。采用这种方法能够加快收敛速度，并减小震荡。 . 公式： . vt=γ∗vt−1+η∗∇θJ(θ)θ=θ−vtv_t = gamma * v_{t-1} + eta * nabla_ theta J( theta) theta = theta - v_tvt​=γ∗vt−1​+η∗∇θ​J(θ)θ=θ−vt​ . pytorch 代码： . for p in group[&#39;params&#39;]: d_p = p.grad.data param_state = self.state[p] if &#39;momentum_buffer&#39; not in param_state: buf = param_state[&#39;momentum_buffer&#39;] = torch.clone(d_p).detach() else: buf = param_state[&#39;momentum_buffer&#39;] buf.mul_(momentum).add_(d_p) d_p = buf p.data.add_(-group[&#39;lr&#39;], d_p) . 其中buf就是vvv，此外，pytorch的官方实现中跟公式的区别在于代码中对于γ∗vt−1 gamma * v_{t-1}γ∗vt−1​项也乘了lr，也就是公式中的η etaη。 . Nesterov Accelerated Gradient (NAG) . 公式： . begin{align} theta^ prime &amp;= theta - gamma v_{t-1} v_t &amp;= gamma v_{t-1} + eta nabla_ theta J( theta^ prime) theta &amp;= theta - v_t end{align} . pytorch 实现公式推导： . begin{align} theta_{t-1}^ prime &amp;= theta_{t-1}- eta gamma v_{t-1} v_t &amp;= gamma v_{t-1} + nabla_ theta J( theta_{t-1}^ prime) theta_t &amp;= theta_{t-1} - eta v_t &amp;= theta_{t-1}^ prime + eta gamma v_{t-1} - eta v_t theta_t^ prime &amp;= theta_t - eta gamma v_t &amp;= theta_{t-1}^ prime + eta gamma v_{t-1} - eta v_t - eta gamma v_t &amp;= theta_{t-1}^ prime - eta nabla_ theta J( theta_{t-1}^ prime) - eta gamma v_t &amp;= theta_{t-1}^ prime - eta( nabla_ theta J( theta_{t-1}^ prime) + gamma v_t) v_{t+1} &amp;= gamma v_t + nabla_ theta J( theta_t^ prime) end{align} . pytorch 代码： . for p in group[&#39;params&#39;]: d_p = p.grad.data param_state = self.state[p] if &#39;momentum_buffer&#39; not in param_state: buf = param_state[&#39;momentum_buffer&#39;] = torch.clone(d_p).detach() else: buf = param_state[&#39;momentum_buffer&#39;] buf.mul_(momentum).add_(1 - dampening, d_p) d_p = d_p.add(momentum, buf) # nesterov更新 p.data.add_(-group[&#39;lr&#39;], d_p) . AdaGrad . AdaGrad是为了解决SGD中学习率固定不变的问题，使得更新的学习率具有自适应能力。具体的想法就是用一个参数来存储历史梯度信息，然后在对应的更新过程中对学习率进行去归一化，从而实现对梯度较大的参数学习率较小，梯度较小的参数学习率较大的自适应更新。 . 公式： . begin{align} G_{t,ii} &amp;= G_{t-1,ii} + g_{t,i}^2 theta_{t+1,i} &amp;= theta_{t,i} - frac{ eta}{ sqrt{G_{t,ii}+ epsilon}} cdot g_{t,i} end{align} . pytorch代码： . for p in group[&#39;params&#39;]: grad = p.grad.data state = self.state[p] state[&#39;step&#39;] += 1 clr = group[&#39;lr&#39;] / (1 + (state[&#39;step&#39;] - 1) * group[&#39;lr_decay&#39;]) state[&#39;sum&#39;].addcmul_(1, grad, grad) # 历史信息更新 std = state[&#39;sum&#39;].sqrt().add_(group[&#39;eps&#39;]) # 自适应调整项 p.data.addcdiv_(-clr, grad, std) . Adadelta . Adadelta是AdaGrad的改进，不再保存过去的所有梯度信息，而是采用decaying average的方法存储历史梯度信息和参数值。 . 公式： . begin{align} E[g^2]t &amp;= rho E[g^2]{t-1} + (1- rho)g^2t Delta x_t &amp;=- frac{ sqrt{E[ Delta x^2]{t-1}+ epsilon}}{ sqrt{E[g^2]t+ epsilon}} cdot g_t E[ Delta x^2]_t &amp;= rho E[ Delta x^2]{t-1} + (1- rho) Delta x^2t x{t+1} &amp;= x_t + Delta x_t end{align} . pytorch 代码： . for p in group[&#39;params&#39;]: grad = p.grad.data state = self.state[p] # State initialization if len(state) == 0: state[&#39;step&#39;] = 0 state[&#39;square_avg&#39;] = torch.zeros_like(p.data, memory_format=torch.preserve_format) state[&#39;acc_delta&#39;] = torch.zeros_like(p.data, memory_format=torch.preserve_format) square_avg, acc_delta = state[&#39;square_avg&#39;], state[&#39;acc_delta&#39;] rho, eps = group[&#39;rho&#39;], group[&#39;eps&#39;] state[&#39;step&#39;] += 1 # 主体更新代码 square_avg.mul_(rho).addcmul_(1 - rho, grad, grad) std = square_avg.add(eps).sqrt_() delta = acc_delta.add(eps).sqrt_().div_(std).mul_(grad) p.data.add_(-group[&#39;lr&#39;], delta) acc_delta.mul_(rho).addcmul_(1 - rho, delta, delta) . RMSprop . RMSprop实际上就是把AdaGrad中历史梯度信息的累积方法改为decaying average方式。 . 公式： . begin{align} E[g^2]t &amp;= rho E[g^2]{t-1} + (1- rho)g^2t theta{t+1} &amp;= theta_t - frac{ eta}{ sqrt{E[g^2]_t+ epsilon}} cdot g_t end{align} . pytorch 代码 . for p in group[&#39;params&#39;]: grad = p.grad.data state = self.state[p] # State initialization if len(state) == 0: state[&#39;step&#39;] = 0 state[&#39;square_avg&#39;] = torch.zeros_like(p.data, memory_format=torch.preserve_format) square_avg = state[&#39;square_avg&#39;] alpha = group[&#39;alpha&#39;] state[&#39;step&#39;] += 1 square_avg.mul_(alpha).addcmul_(1 - alpha, grad, grad) avg = square_avg.sqrt().add_(group[&#39;eps&#39;]) p.data.addcdiv_(-group[&#39;lr&#39;], grad, avg) . pytorch 官方实现中还有一个可选参数centered，表示一个centered版本的RMSprop，其更新公式如下： . begin{align} E[g^2]t &amp;= rho E[g^2]{t-1} + (1- rho)g^2t E[g]_t &amp;= rho E[g]{t-1} + (1- rho)g_t theta_{t+1} &amp;= theta_t - frac{ eta}{ sqrt{E[g^2]_t - E[g]_t^2 + epsilon}} cdot g_t end{align} . Adam (Adaptive Moment Estimation) . Adam受到RMSprop和Momentum的启发，利用梯度的一阶和二阶矩估计为每个参数自适应地计算学习率。 . 公式： . begin{align} alpha:learning rate; quad beta_1&amp;=0.9, beta_2=0.999; quad t:timestep m_t &amp;= beta_1 m_{t-1} + (1- beta_1) cdot g_t v_t &amp;= beta_2 v_{t-1} + (1- beta_2) cdot g_t^2 hat{m}t &amp;= frac{m_t}{1- beta_1^t} hat{v}_t &amp;= frac{v_t}{1- beta_2^t} theta_t &amp;= theta{t-1} - alpha cdot frac{ hat{m}_t}{ sqrt{ hat{v}_t}+ epsilon} end{align} . pytorch代码： . for p in group[&#39;params&#39;]: grad = p.grad.data state = self.state[p] # State initialization if len(state) == 0: state[&#39;step&#39;] = 0 # Exponential moving average of gradient values state[&#39;exp_avg&#39;] = torch.zeros_like(p.data, memory_format=torch.preserve_format) # Exponential moving average of squared gradient values state[&#39;exp_avg_sq&#39;] = torch.zeros_like(p.data, memory_format=torch.preserve_format) exp_avg, exp_avg_sq = state[&#39;exp_avg&#39;], state[&#39;exp_avg_sq&#39;] beta1, beta2 = group[&#39;betas&#39;] state[&#39;step&#39;] += 1 bias_correction1 = 1 - beta1 ** state[&#39;step&#39;] bias_correction2 = 1 - beta2 ** state[&#39;step&#39;] # Decay the first and second moment running average coefficient exp_avg.mul_(beta1).add_(1 - beta1, grad) exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad) denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group[&#39;eps&#39;]) step_size = group[&#39;lr&#39;] / bias_correction1 p.data.addcdiv_(-step_size, exp_avg, denom) . pytorch官方代码中采用了跟论文方法等效的一种修改的实现方式，修改的更新公式如下： . begin{align} alpha_t &amp;= alpha cdot frac{ sqrt{1- beta^t_2}}{1- beta_1^t} theta_t &amp;= theta_{t-1} - alpha_t cdot frac{m_t}{ sqrt{v_t}+ epsilon} end{align} . Adam with Weight Decay . 这是一篇解释AdamW的好文章：Why AdamW matters。 . AdamW通过正确的weight修正方式，弥补了Adam with L2 Regularization的不足，实验结果表明AdamW可以实现快速收敛，这里有一篇fast.ai上的分析文章：AdamW and Super-convergence is now the fastest way to train neural nets。 . 从下面的图中可以清楚看到L2正则化与Weight decay更新的区别。L2正则化因为是在Loss function中加入正则化项，不仅影响梯度的计算，也影响了$m_t$和$v_t$这两个保存梯度历史信息的参数。而weight decay是在参数更新的时候直接进行计算，不会将weight decay的影响带入其他地方。 . . pytorch 代码：（代码跟Adam的代码相比只是多了一行进行weight decay的计算，其他的完全一样） . for p in group[&#39;params&#39;]: # Perform stepweight decay p.data.mul_(1 - group[&#39;lr&#39;] * group[&#39;weight_decay&#39;]) grad = p.grad.data # ... # 同Adam代码 . 参考文献 . 梯度下降优化算法综述与PyTorch实现源码剖析 | An overview of gradient descent optimization algorithms | AdamW and Super-convergence is now the fastest way to train neural nets | Why AdamW matters |",
            "url": "https://leiyu-thunder.github.io/myblog/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2020/02/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96%E5%99%A8-pytorch.html",
            "relUrl": "/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2020/02/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96%E5%99%A8-pytorch.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Pytorch_notes",
            "content": "pytorch notes . detach的作用 . 在optimizer的代码中，将梯度置零的方法是： . p.grad.detach_() p.grad.zero_() . 困惑：第二行代码就可以实现置零，第一行代码用来干什么？ . 解答：第一行代码用于将之前的梯度从计算图中剥离。pytorch中梯度计算的方式是将之前的梯度叠加，如果不剥离梯度，只是置零，那么这个梯度一直需要保存，所以如果计算了100次，那么就需要100倍的空间来保存这些全部是零的结果，导致空间浪费。 .",
            "url": "https://leiyu-thunder.github.io/myblog/pytorch/2020/02/17/pytorch_notes.html",
            "relUrl": "/pytorch/2020/02/17/pytorch_notes.html",
            "date": " • Feb 17, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://leiyu-thunder.github.io/myblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://leiyu-thunder.github.io/myblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}