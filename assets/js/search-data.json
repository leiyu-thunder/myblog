{
  
    
        "post0": {
            "title": "Vim使用笔记",
            "content": "vim使用笔记 . 常规文本操作 . 删除、撤销、复制、粘贴等 . u撤销操作 | Ctrl-r重做操作 | :↑在切换上一个命令 | yy复制 | p粘贴（p/P都可以，p是表⽰在当前位置之后，P表⽰在当前位置之前）） | dd删除当前行 | . 各种插入模式 . a → 在光标后插⼊ | o → 在当前⾏后插⼊⼀个新⾏ | O → 在当前⾏前插⼊⼀个新⾏ | . 光标移动 . hjkl(←↓↑→）的移动 . | 0 → 数字零，到⾏头 . | ^ → 到本⾏第⼀个不是blank字符的位置（所谓blank字符就是空格， tab，换⾏，回⻋等） . | $ → 到本⾏⾏尾 . | g_ → 到本⾏最后⼀个不是blank字符的位置。 . | NG → 到第 N ⾏ . | gg → 到第⼀⾏ . | G → 到最后⼀⾏ . | w → 到下⼀个单词的开头 . | e → 到下⼀个单词的结尾 . 如果你认为单词是由默认⽅式，那么就⽤⼩写的e和w。默认上来说，⼀个 单词由字⺟，数字和下划线组成 . 如果你认为单词是由blank字符分隔符，那么你需要使⽤⼤写的E和W。 . | b → 倒退到上一个单词开头 . | % →匹配括号移动 . | *和 #：匹配光标当前所在的单词，移动光标到下⼀个（或上⼀个）匹配单词（*是下⼀个，#是上⼀个）。 . | H → 移动到当前屏幕第一行 . | M → 移动到当前屏幕中间行 . | L → 移动到当前屏幕最后一行 . | . 屏幕滚动 . Ctrl-F →屏幕向下滚动一屏 | Ctrl-B→屏幕向上滚动一屏 | Ctrl-E→屏幕向下滚动一行 | Ctrl-Y→屏幕向上滚动一行 | Ctrl-D→屏幕向下滚动半屏 | Ctrl-U→屏幕向上滚动半屏 | . 文件操作 . :e &lt;path/to/file&gt; → 打开⼀个⽂件 | :w → 存盘 | :saveas &lt;path/to/file&gt; → 另存为 &lt;path/to/file&gt; | :x， ZZ 或 :wq → 保存并退出 (:x 表⽰仅在需要时保存，ZZ不需要输 ⼊冒号并回⻋) :q! → 退出不保存 :qa! 强⾏退出所有的正在编辑的⽂件，就算别的⽂件有更改。 | . 重复命令 . . → (⼩数点) 可以重复上⼀次的命令 | :N&lt;command&gt; → 重复某个命令N次 | . 不同buffer之间跳转 . :bn切换到下一个文件。 | :bp切换到上一个文件。 | :ls列出buffer的列表，带编号。 | :b [N]切换到第N个文件，例如：:b 3切换到第三个文件。 | :b {filename}通过文件名切换到对应的buffer，tab可以补全。 | . 多窗口与窗口跳转 . :sp打开一个新的水平切分窗口。 | :vsplit打开一个新的垂直切分窗口。 | . 打开多个窗口后，以下命令在多窗口之间跳转（类似鼠标上下左右移动）： . Ctrl-W j切换到下一个窗口 | Ctrl-W k切换到上一个窗口 | Ctrl-W h切换到左边窗口 | Ctrl-W l切换到右边窗口 | Ctrl-W w在各个窗口之间轮流切换 | . 代码跳转 . 在目标文件夹下生成代码标签：ctags -R | 通过vim打开某一代码文件，把光标移到要跳转的标识符上，按Ctrl + ]跳转到对应位置，按Ctrl + o退回到跳转前的位置 | 开启代码折叠：:set foldmethod=indent zM 全部折叠 | zR 全部展开 | zc 在当前位置折叠 | zo | za 在当前位置展开 | zC 对当前区域做整体折叠；当前区域可以理解为整个函数 | zO | zA 对当前区域做整体展开 | zj 移动下一个折叠点 | zk 移动到上一个折叠垫 | . | . 文本搜索与替换 . /pattern在文本中按照pattern模式进行搜索，n查找下一个，N查找上一个 | *可以查找光标所在的单词，g*即可查找光标所在单词的字符序列，每次出现前后字符无要求。 | s命令用来查找和替换字符串，语法：:{作用范围}s/{目标}/{替换}/{替换标志} | . ​ 例如:%s/foo/bar/g会在全局范围(%)查找foo并替换为bar，每一行中所有出现都会被替换（g）。 . ​ 不同的作用范围标志： . 命令 作用范围 . :s/foo/bar/g | 当前行 | . :%s/foo/bar/g | 全文 | . :’&lt;,’&gt;s/foo/bar/g | visual模式下的选择区域，在visual模式下选择区域后输入:即可自动补全 | . :.,+2s/foo/bar/g | 当前行与接下来的两行 | . 不同的替换标志：g全局替换 . | 全局命令:g，语法：:[range]global[!]/{pattern}/{command}，简写成:[range]g/pattern/command。 . [range] 指定文本范围，默认为整个文档 . | pattern 在范围 range 内的行如果匹配 pattern，则执行 command | ! 表示取反，也就是不匹配的行，也可以使用 vglobal | command 默认是打印文本 | 整个命令可以理解成，在 range 范围内匹配 patter 的行执行 Ex command。 | 常用的Ex command：d（删除）、m（移动）、t（拷贝）、s（替换） | 删除匹配的行：:g/pattern/d | 删除不匹配的行：:g!/pattern/d | 删除空白行：:g/^$/d | 将匹配的行移到文件末尾：g/pattern/m$ | 复制匹配的行到文件末尾：g/pattern/t$ | . | . 正则表达式 . 正则表达式的元字符必须用反斜杠进行转义才行 | 非贪婪匹配：. {-}，贪婪匹配：.* | . 使用终端 . !{cmd}是vim命令，可以用来执行一条shell命令，命令完成后按任意键返回vim。 | Ctrl+z可以立即挂起当前进程，比如vim，进入shell， 完成一系列命令后，使用fg来返回vim。 | :sh[ell]是vim命令，可以从vim中运行一个shell，完成一系列命令后，通过Ctrl+d结束当前shell并返回vim。 | . vim 插件管理 . 常用的vim插件管理方法是vundle。 | . 其他命令 . Ctrl -缩小文本字号 | Ctrl +放大文本字号 | | .",
            "url": "https://leiyu-thunder.github.io/myblog/vim/2020/02/24/vim%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0.html",
            "relUrl": "/vim/2020/02/24/vim%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0.html",
            "date": " • Feb 24, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "深度学习优化器（pytorch）",
            "content": "常用深度学习优化器：理论与pytorch代码 . SGD(Stochastic Gradient Descent) . 随机梯度下降中的”stochastic”表示用小批量数据代表模型梯度更新方向。 . 公式： . θ=θ−η∗∇θJ(θ;xi;yi) theta = theta - eta * nabla_ theta J( theta;x^i;y^i)θ=θ−η∗∇θ​J(θ;xi;yi) . pytorch 代码： . for p in group[&#39;params&#39;]: d_p = p.grad.data p.data.add_(-group[&#39;lr&#39;], d_p) . SGD with momentum . SGD的缺点之一是容易在局部极值点反复震荡，解决方法之一在更新过程中加入历史更新信息。类似于从山上滚下的石头，没有阻力的话动量越来越大，遇到阻力动量变小。采用这种方法能够加快收敛速度，并减小震荡。 . 公式： . vt=γ∗vt−1+η∗∇θJ(θ)θ=θ−vtv_t = gamma * v_{t-1} + eta * nabla_ theta J( theta) theta = theta - v_tvt​=γ∗vt−1​+η∗∇θ​J(θ)θ=θ−vt​ . pytorch 代码： . for p in group[&#39;params&#39;]: d_p = p.grad.data param_state = self.state[p] if &#39;momentum_buffer&#39; not in param_state: buf = param_state[&#39;momentum_buffer&#39;] = torch.clone(d_p).detach() else: buf = param_state[&#39;momentum_buffer&#39;] buf.mul_(momentum).add_(d_p) d_p = buf p.data.add_(-group[&#39;lr&#39;], d_p) . 其中buf就是vvv，此外，pytorch的官方实现中跟公式的区别在于代码中对于γ∗vt−1 gamma * v_{t-1}γ∗vt−1​项也乘了lr，也就是公式中的η etaη。 . Nesterov Accelerated Gradient (NAG) . 公式： . begin{align} theta^ prime &amp;= theta - gamma v_{t-1} v_t &amp;= gamma v_{t-1} + eta nabla_ theta J( theta^ prime) theta &amp;= theta - v_t end{align} . pytorch 实现公式推导： . begin{align} theta_{t-1}^ prime &amp;= theta_{t-1}- eta gamma v_{t-1} v_t &amp;= gamma v_{t-1} + nabla_ theta J( theta_{t-1}^ prime) theta_t &amp;= theta_{t-1} - eta v_t &amp;= theta_{t-1}^ prime + eta gamma v_{t-1} - eta v_t theta_t^ prime &amp;= theta_t - eta gamma v_t &amp;= theta_{t-1}^ prime + eta gamma v_{t-1} - eta v_t - eta gamma v_t &amp;= theta_{t-1}^ prime - eta nabla_ theta J( theta_{t-1}^ prime) - eta gamma v_t &amp;= theta_{t-1}^ prime - eta( nabla_ theta J( theta_{t-1}^ prime) + gamma v_t) v_{t+1} &amp;= gamma v_t + nabla_ theta J( theta_t^ prime) end{align} . pytorch 代码： . for p in group[&#39;params&#39;]: d_p = p.grad.data param_state = self.state[p] if &#39;momentum_buffer&#39; not in param_state: buf = param_state[&#39;momentum_buffer&#39;] = torch.clone(d_p).detach() else: buf = param_state[&#39;momentum_buffer&#39;] buf.mul_(momentum).add_(1 - dampening, d_p) d_p = d_p.add(momentum, buf) # nesterov更新 p.data.add_(-group[&#39;lr&#39;], d_p) . AdaGrad . AdaGrad是为了解决SGD中学习率固定不变的问题，使得更新的学习率具有自适应能力。具体的想法就是用一个参数来存储历史梯度信息，然后在对应的更新过程中对学习率进行去归一化，从而实现对梯度较大的参数学习率较小，梯度较小的参数学习率较大的自适应更新。 . 公式： . begin{align} G_{t,ii} &amp;= G_{t-1,ii} + g_{t,i}^2 theta_{t+1,i} &amp;= theta_{t,i} - frac{ eta}{ sqrt{G_{t,ii}+ epsilon}} cdot g_{t,i} end{align} . pytorch代码： . for p in group[&#39;params&#39;]: grad = p.grad.data state = self.state[p] state[&#39;step&#39;] += 1 clr = group[&#39;lr&#39;] / (1 + (state[&#39;step&#39;] - 1) * group[&#39;lr_decay&#39;]) state[&#39;sum&#39;].addcmul_(1, grad, grad) # 历史信息更新 std = state[&#39;sum&#39;].sqrt().add_(group[&#39;eps&#39;]) # 自适应调整项 p.data.addcdiv_(-clr, grad, std) . Adadelta . Adadelta是AdaGrad的改进，不再保存过去的所有梯度信息，而是采用decaying average的方法存储历史梯度信息和参数值。 . 公式： . begin{align} E[g^2]t &amp;= rho E[g^2]{t-1} + (1- rho)g^2t Delta x_t &amp;=- frac{ sqrt{E[ Delta x^2]{t-1}+ epsilon}}{ sqrt{E[g^2]t+ epsilon}} cdot g_t E[ Delta x^2]_t &amp;= rho E[ Delta x^2]{t-1} + (1- rho) Delta x^2t x{t+1} &amp;= x_t + Delta x_t end{align} . pytorch 代码： . for p in group[&#39;params&#39;]: grad = p.grad.data state = self.state[p] # State initialization if len(state) == 0: state[&#39;step&#39;] = 0 state[&#39;square_avg&#39;] = torch.zeros_like(p.data, memory_format=torch.preserve_format) state[&#39;acc_delta&#39;] = torch.zeros_like(p.data, memory_format=torch.preserve_format) square_avg, acc_delta = state[&#39;square_avg&#39;], state[&#39;acc_delta&#39;] rho, eps = group[&#39;rho&#39;], group[&#39;eps&#39;] state[&#39;step&#39;] += 1 # 主体更新代码 square_avg.mul_(rho).addcmul_(1 - rho, grad, grad) std = square_avg.add(eps).sqrt_() delta = acc_delta.add(eps).sqrt_().div_(std).mul_(grad) p.data.add_(-group[&#39;lr&#39;], delta) acc_delta.mul_(rho).addcmul_(1 - rho, delta, delta) . RMSprop . RMSprop实际上就是把AdaGrad中历史梯度信息的累积方法改为decaying average方式。 . 公式： . begin{align} E[g^2]t &amp;= rho E[g^2]{t-1} + (1- rho)g^2t theta{t+1} &amp;= theta_t - frac{ eta}{ sqrt{E[g^2]_t+ epsilon}} cdot g_t end{align} . pytorch 代码 . for p in group[&#39;params&#39;]: grad = p.grad.data state = self.state[p] # State initialization if len(state) == 0: state[&#39;step&#39;] = 0 state[&#39;square_avg&#39;] = torch.zeros_like(p.data, memory_format=torch.preserve_format) square_avg = state[&#39;square_avg&#39;] alpha = group[&#39;alpha&#39;] state[&#39;step&#39;] += 1 square_avg.mul_(alpha).addcmul_(1 - alpha, grad, grad) avg = square_avg.sqrt().add_(group[&#39;eps&#39;]) p.data.addcdiv_(-group[&#39;lr&#39;], grad, avg) . pytorch 官方实现中还有一个可选参数centered，表示一个centered版本的RMSprop，其更新公式如下： . begin{align} E[g^2]t &amp;= rho E[g^2]{t-1} + (1- rho)g^2t E[g]_t &amp;= rho E[g]{t-1} + (1- rho)g_t theta_{t+1} &amp;= theta_t - frac{ eta}{ sqrt{E[g^2]_t - E[g]_t^2 + epsilon}} cdot g_t end{align} . Adam (Adaptive Moment Estimation) . Adam受到RMSprop和Momentum的启发，利用梯度的一阶和二阶矩估计为每个参数自适应地计算学习率。 . 公式： . begin{align} alpha:learning rate; quad beta_1&amp;=0.9, beta_2=0.999; quad t:timestep m_t &amp;= beta_1 m_{t-1} + (1- beta_1) cdot g_t v_t &amp;= beta_2 v_{t-1} + (1- beta_2) cdot g_t^2 hat{m}t &amp;= frac{m_t}{1- beta_1^t} hat{v}_t &amp;= frac{v_t}{1- beta_2^t} theta_t &amp;= theta{t-1} - alpha cdot frac{ hat{m}_t}{ sqrt{ hat{v}_t}+ epsilon} end{align} . pytorch代码： . for p in group[&#39;params&#39;]: grad = p.grad.data state = self.state[p] # State initialization if len(state) == 0: state[&#39;step&#39;] = 0 # Exponential moving average of gradient values state[&#39;exp_avg&#39;] = torch.zeros_like(p.data, memory_format=torch.preserve_format) # Exponential moving average of squared gradient values state[&#39;exp_avg_sq&#39;] = torch.zeros_like(p.data, memory_format=torch.preserve_format) exp_avg, exp_avg_sq = state[&#39;exp_avg&#39;], state[&#39;exp_avg_sq&#39;] beta1, beta2 = group[&#39;betas&#39;] state[&#39;step&#39;] += 1 bias_correction1 = 1 - beta1 ** state[&#39;step&#39;] bias_correction2 = 1 - beta2 ** state[&#39;step&#39;] # Decay the first and second moment running average coefficient exp_avg.mul_(beta1).add_(1 - beta1, grad) exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad) denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group[&#39;eps&#39;]) step_size = group[&#39;lr&#39;] / bias_correction1 p.data.addcdiv_(-step_size, exp_avg, denom) . pytorch官方代码中采用了跟论文方法等效的一种修改的实现方式，修改的更新公式如下： . begin{align} alpha_t &amp;= alpha cdot frac{ sqrt{1- beta^t_2}}{1- beta_1^t} theta_t &amp;= theta_{t-1} - alpha_t cdot frac{m_t}{ sqrt{v_t}+ epsilon} end{align} . Adam with Weight Decay . 这是一篇解释AdamW的好文章：Why AdamW matters。 . AdamW通过正确的weight修正方式，弥补了Adam with L2 Regularization的不足，实验结果表明AdamW可以实现快速收敛，这里有一篇fast.ai上的分析文章：AdamW and Super-convergence is now the fastest way to train neural nets。 . 从下面的图中可以清楚看到L2正则化与Weight decay更新的区别。L2正则化因为是在Loss function中加入正则化项，不仅影响梯度的计算，也影响了$m_t$和$v_t$这两个保存梯度历史信息的参数。而weight decay是在参数更新的时候直接进行计算，不会将weight decay的影响带入其他地方。 . . pytorch 代码：（代码跟Adam的代码相比只是多了一行进行weight decay的计算，其他的完全一样） . for p in group[&#39;params&#39;]: # Perform stepweight decay p.data.mul_(1 - group[&#39;lr&#39;] * group[&#39;weight_decay&#39;]) grad = p.grad.data # ... # 同Adam代码 . 参考文献 . 梯度下降优化算法综述与PyTorch实现源码剖析 | An overview of gradient descent optimization algorithms | AdamW and Super-convergence is now the fastest way to train neural nets | Why AdamW matters |",
            "url": "https://leiyu-thunder.github.io/myblog/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2020/02/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96%E5%99%A8-pytorch.html",
            "relUrl": "/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2020/02/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96%E5%99%A8-pytorch.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . Front Matter is a markdown cell at the beginning of your notebook that allows you to inject metadata into your notebook. For example: . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks just like you can with markdown. . For example, here is a footnote 1. . . This is the footnote.&#8617; . |",
            "url": "https://leiyu-thunder.github.io/myblog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Pytorch_notes",
            "content": "pytorch notes . detach的作用 . 在optimizer的代码中，将梯度置零的方法是： . p.grad.detach_() p.grad.zero_() . 困惑：第二行代码就可以实现置零，第一行代码用来干什么？ . 解答：第一行代码用于将之前的梯度从计算图中剥离。pytorch中梯度计算的方式是将之前的梯度叠加，如果不剥离梯度，只是置零，那么这个梯度一直需要保存，所以如果计算了100次，那么就需要100倍的空间来保存这些全部是零的结果，导致空间浪费。 .",
            "url": "https://leiyu-thunder.github.io/myblog/pytorch/2020/02/17/pytorch_notes.html",
            "relUrl": "/pytorch/2020/02/17/pytorch_notes.html",
            "date": " • Feb 17, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Test Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://leiyu-thunder.github.io/myblog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://leiyu-thunder.github.io/myblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}